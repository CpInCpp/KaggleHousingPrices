{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from modelling_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sama/Bootcamp/Project/machina_ex_kaggle/modelling/modelling_functions.py:109: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  housing.col, id_dictonary = to_numeric(housing, col, 'SalePrice')\n"
     ]
    }
   ],
   "source": [
    "#### Read Data files\n",
    "housing, housing_features, feat_labels, dict_dictonary = read_and_clean(filepath = \"../data/clean_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sama/Bootcamp/Project/machina_ex_kaggle/modelling/modelling_functions.py:92: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  housing.col, id_dictonary = to_numeric_test(housing, col, dictonary[col])\n"
     ]
    }
   ],
   "source": [
    "htest_id, htest_features, htest_labels, htest_dictonary = read_and_clean(filepath = \"../data/clean_test.csv\", test = True, dictonary = dict_dictonary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_features.lotarea = np.sqrt(housing_features.lotarea)\n",
    "htest_features.lotarea = np.sqrt(htest_features.lotarea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "htrain, htest, ptrain, ptest = train_test_split(housing_features, housing.saleprice, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 1288, 2377, 3466, 4555, 5644, 6733, 7822, 8911, 10000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 29, 48, 67, 86, 105, 124, 143, 162, 181, 200, None], 'min_samples_split': [2, 5, 10, 15, 20, 30, 40], 'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 10000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 200, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15, 20, 30, 40]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 8, 16, 32, 64]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 353 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 636 tasks      | elapsed: 22.4min\n",
      "C:\\Users\\Olympus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 1001 tasks      | elapsed: 37.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1446 tasks      | elapsed: 55.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1973 tasks      | elapsed: 76.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2580 tasks      | elapsed: 99.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 115.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=1000, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 1288, 2377, 3466, 4555, 5644, 6733, 7822, 8911, 10000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 29, 48, 67, 86, 105, 124, 143, 162, 181, 200, None], 'min_samples_split': [2, 5, 10, 15, 20, 30, 40], 'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 1000, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(htrain, ptrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 4555,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 162,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators=4555, \n",
    "                            random_state=9, \n",
    "                            n_jobs=-1, \n",
    "                            min_samples_split = 2, \n",
    "                            min_samples_leaf = 1, \n",
    "                            max_features = 'sqrt', \n",
    "                            max_depth = 162,\n",
    "                            bootstrap = False)\n",
    "#sfm = SelectFromModel(clf, threshold = 0.01)\n",
    "#sfm.fit(htrain, ptrain)\n",
    "# ## Not Finished\n",
    "# # Measure Feature Importance\n",
    "#feature_selected = []\n",
    "#for feature_list_index in sfm.get_support(indices=True):\n",
    "#     feature_selected.append(feat_labels[feature_list_index])\n",
    "#proxy = feature_selected\n",
    "#trimmed = ['x1stflrsf', 'x2ndflrsf', 'garagecars', 'overallcond', 'saleprice', 'Unnamed: 0', 'bsmtfinsf1']\n",
    "#testing = testing[proxy]\n",
    "#housing_features = housing_features[trimmed_features]\n",
    "#print(feature_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8687705633125573"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(htrain, ptrain)\n",
    "\n",
    "clf.score(htest, ptest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019993873280056498"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nice = clf.predict(htest)\n",
    "mean_squared_error(nice, ptest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.exp(clf.predict(htest_features))\n",
    "Submission(htest_id, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "alp = optimize_penalty(htrain, ptrain, model=Lasso, min_=1e-5, max_=1, step_=1000, random=False, riter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1e-05}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alp.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-04, 1.01009901e-02, 2.01019802e-02, ...,\n",
       "       9.99799980e+01, 9.99899990e+01, 1.00000000e+02])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(1e-10, 1, num = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9106296982946355\n",
      "0.8370981117816123\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso()\n",
    "lasso.set_params(alpha = 1e-5)\n",
    "lasso.fit(htrain, ptrain)\n",
    "\n",
    "print(lasso.score(htrain, ptrain))\n",
    "print(lasso.score(htest, ptest))\n",
    "#print(selected_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.exp(lasso.predict(htest_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission(htest_id, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
